import datetime
import pandas as pd

import great_expectations as gx
import great_expectations.jupyter_ux
from great_expectations.core.batch import BatchRequest
from great_expectations.checkpoint import SimpleCheckpoint
from great_expectations.exceptions import DataContextError

# Load the Great Expectations context

context = gx.get_context()

batch_request = {'datasource_name': 'my_datasource', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'gx_new .csv', 'limit': 1000}

expectation_suite_name = "great_profiling"

validator = context.get_validator(
    batch_request=BatchRequest(**batch_request),
    expectation_suite_name=expectation_suite_name
)


# Load a batch of data
batch_data = validator.head(n_rows=1000, fetch_all=False)

# Exclude 'Name' column from numeric calculations or expectations
exclude_column_names = ["Name"]

# Calculate the row count
row_count = len(batch_data)
print("Row Count:", row_count)

# Retrieve column metrics and add data profiling expectations for each column
for column in batch_data.columns:
    if column not in exclude_column_names:
        # Retrieve non-null count and unique count
        non_null_count = batch_data[column].count()
        unique_count = batch_data[column].nunique()

        print(f"Column: {column}, Non-null Count: {non_null_count}, Unique Count: {unique_count}")

        # Add data profiling expectations
        validator.expect_column_mean_to_be_between(column=column, min_value=0, max_value=None)
        validator.expect_column_sum_to_be_between(column=column)
        validator.expect_column_median_to_be_between(column=column)
        validator.expect_column_min_to_be_between(column=column)
        validator.expect_column_max_to_be_between(column=column)
        validator.expect_column_stdev_to_be_between(column=column)
        validator.expect_column_unique_value_count_to_be_between(column=column)
        
        validator.expect_column_value_lengths_to_be_between(column=column, min_value=0, max_value=None)
        # Add more expectations as needed

# Save the expectation suite
validator.save_expectation_suite(discard_failed_expectations=False)

# Create a SimpleCheckpoint to track validation results
checkpoint_config = {
    "class_name": "SimpleCheckpoint",
    "validations": [
        {
            "batch_request": batch_request,
            "expectation_suite_name": expectation_suite_name
        }
    ]
}
checkpoint = SimpleCheckpoint(
    f"{validator.active_batch_definition.data_asset_name}_{expectation_suite_name}",
    context,
    **checkpoint_config
)

# Run the checkpoint
checkpoint_result = checkpoint.run()

# Build data docs
context.build_data_docs()

# Open data docs
validation_result_identifier = checkpoint_result.list_validation_result_identifiers()[0]
context.open_data_docs(resource_identifier=validation_result_identifier)




**************************************************

